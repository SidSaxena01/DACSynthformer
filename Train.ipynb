{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "581d5aad",
   "metadata": {},
   "source": [
    "### chaptGPT specs   \n",
    "\n",
    "A decoder-only transformer in pytorch to predict 'next output' at each time step. \n",
    "\n",
    "Each time step t is represented by a vector of n=4 tokens. \n",
    "The length of the sequence (context window) is Ti=86 for inference, and Tt=8*Ti for training. That is, the context window for training is 8 times the length of the context window for inference. \n",
    "The attention is \"causal\", looking only back in time, and the maximum look-back time for the attention blocks is Ti (even when the sequence is longer during training).\n",
    "\n",
    "The size of the vocabulary for each of the n tokens is V=1024. \n",
    "\n",
    "The dataloader will as is usual, supply batches pairs of (input,target) where the size of each input and output is Tt*n (the sequence length times the number of tokens at each time step). The tokens are indexes for the vocabulary in the range of (0, V-1). The targets are shifted relative to the input sequence by 1 as is typical for networks the learn to predict the output for the next time step. \n",
    "\n",
    "The first layer in the architecture will be a learnable \"multiembedding\" layer that embeds each of the 4 tokens at each time step as an 8-dimensional vector. The n 8-dimensional vectors is concatenated to provide the 32 dimensional input for the transformer blocks at each time step. \n",
    "\n",
    "A positional code is then added to each 32 dimensional vector. For positional encoding, use Rotary Position Embedding (RoPE).\n",
    "\n",
    "We use a stack of b=8 transformer blocks that are standard (using layer norms, a relu for activation, and a forward expansion factor of 4 form the linear layer). Each transformer block consumes and produces a context window length sequence of 32 dimensional vectors. \n",
    "\n",
    "After the last transformer block, there should be linear layer that maps the 32 dimensional vectors to the output size which is V*n (the vocabulary size time the number of tokens stacked at each time step). These are the logits that will be fed to the softmax functions (one for each of the n tokens) that provide the probability distribtion across the vocabulary set. Use the criterion nn.CrossEntropyLoss() for computing the loss using the targets provided by the dataloader, and Adam for the optimizer.\n",
    "\n",
    "Again, at inference time, the fixed-length context window is shorter than the training sequence window length, and equal to the maximum look-back time of the attention blocks. The inference process should take the output produced at each time step (a stack of n tokens), and shift them in to a sliding window that is used for input for the next time step. Please use the \"Incremental Token Generation Using Cached States\" approach to minimize the computational burden of the shifting input window. Please be careful that it works correctly with the RoPE positional coding. The length of the sequences generated during inference is arbitrary and should be settable with a parameter. \n",
    "\n",
    "Include an example (with dummy inputs and targets) of how to call the code for a training step, and an example of inference for a specifiable output length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bafd951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# and for creating a custom dataset and loader:\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "\n",
    "from utils.utils import generate_mask, save_model, writeDACFile, interpolate_vectors\n",
    "from DACTransformer.DACTransformer import TransformerDecoder\n",
    "from dataloader.dataset import CustomDACDataset, onehot, getNumClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a754a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ea333",
   "metadata": {},
   "source": [
    "### <font color='blue'> Parameters \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25994933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basefname = bees.e504.l2.h4\n",
      "outdir = runs/newscratchbeescond8\n"
     ]
    }
   ],
   "source": [
    "# Training data dir\n",
    "\n",
    "testsnd=\"bees\" # pistons, wind, applause, bees\n",
    "# data_dir=\"/scratch/syntex/PisWinAppBee_long_44/onesnddac/\"+testsnd+\"-train-small\"  ##******* small\n",
    "# validator_data_dir=\"/scratch/syntex/PisWinAppBee_long_44/onesnddac/\"+testsnd+\"-val-small\"\n",
    "\n",
    "data_dir=validator_data_dir=\"/scratch/syntex/PisWinAppBee_long_44/onesnddac/\"+testsnd+\"-train-small\"\n",
    "validator_data_dir=\"/scratch/syntex/PisWinAppBee_long_44/onesnddac/\"+testsnd+\"-val-small\"\n",
    "\n",
    "# ---------     for the transformr  --------------#\n",
    "vocab_size = 1024\n",
    "num_tokens = 4\n",
    "\n",
    "cond_classes = 7 #getNumClasses()\n",
    "cond_params = 1 #1\n",
    "cond_size = cond_classes + cond_params # num_classes + num params - not a FREE parameter!\n",
    "embed_size = 512 -cond_size # 240 #32  # embed_size must be divisible by num_heads and by num tokens\n",
    "Ti = 86\n",
    "Tt = 430 # must match the length of the sequences in the batch\n",
    "batch_size = 6\n",
    "sequence_length = Tt  # For training\n",
    "\n",
    "num_layers=2\n",
    "num_heads=4 # 8 # embed_size must be divisible by num_heads\n",
    "forward_expansion=4 #4\n",
    "dropout_rate=0.1\n",
    "learning_rate=0.001\n",
    "\n",
    "top_n = 5   # not used yet\n",
    "\n",
    "num_epochs=100\n",
    "experiment_name='newscratch' + testsnd + 'cond' + str(cond_size)  #the higher the embed size (to 240 anyway) the fewer epochs are necessary\n",
    "outdir = 'runs' + '/' + experiment_name\n",
    "basefname=testsnd+ '.e' + str(embed_size) + '.l' + str(num_layers) + '.h' + str(num_heads) \n",
    "\n",
    "DEVICE='cuda'#My experiments show CUDA is only) 4 times faster than CPU!\n",
    "ALSO_RUN_DUMMY_STEP =False # there a two cells that setup a single dummy input and target and take one training step\n",
    "\n",
    "inference_steps=86*10 # second fact is number of seconds (for 86 tokens/sec)\n",
    "\n",
    "ErrorLogRate=10\n",
    "checkpoint_interval=25\n",
    "\n",
    "# For inference (does an interpolation between min to max and back to min)\n",
    "infminpval=.5\n",
    "infmaxpval=.6\n",
    "\n",
    "print(f'basefname = {basefname}')\n",
    "print(f'outdir = {outdir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e94b4f",
   "metadata": {},
   "source": [
    "### <font color='blue'> Set up cuda. \n",
    "Without it, training runs about 4 times slower  \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5c0e7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memeory on cuda 0 is  25.216745472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_properties(0).total_memory/1e9\n",
    "\n",
    "device = torch.device(DEVICE) # if the docker was started with --gpus all, then can choose here with cuda:0 (or cpu)\n",
    "torch.cuda.device_count()\n",
    "print(f'memeory on cuda 0 is  {torch.cuda.get_device_properties(0).total_memory/1e9}')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b991b4c",
   "metadata": {},
   "source": [
    "### <font color='blue'> Load data \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a3792b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Inputs shape: torch.Size([6, 430, 4])\n",
      "Targets shape: torch.Size([6, 430, 4])\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the dataset\n",
    "dataset = CustomDACDataset(data_dir=data_dir)\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Validator data set\n",
    "if validator_data_dir != None :\n",
    "    validator_dataset=CustomDACDataset(data_dir=validator_data_dir)\n",
    "    validator_dataloader= DataLoader(validator_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "# Test data dir\n",
    "for batch_idx, (inputs, targets, cvect) in enumerate(dataloader):\n",
    "    #pass\n",
    "    # Your training code here\n",
    "    # inputs: batch of input data of shape [batch_size, N, T-1]\n",
    "    # targets: corresponding batch of target data of shape [batch_size, N, T-1]\n",
    "    \n",
    "    if (batch_idx == 0) : \n",
    "        print(f\"Batch {batch_idx + 1}\")\n",
    "        print(f\"Inputs shape: {inputs.shape}\")\n",
    "        print(f\"Targets shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0869e8d9",
   "metadata": {},
   "source": [
    "### <font color='blue'> Instantiate model \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e11b432e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask.shape is torch.Size([430, 430])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [-inf, -inf, -inf,  ..., 0., -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., 0., 0., -inf],\n",
       "        [-inf, -inf, -inf,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = generate_mask(Tt, Ti).to(device)\n",
    "print(f'Mask.shape is {mask.shape}')\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95b9b9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model with embed_size=504, cond_size=8\n",
      "Get a coder with embed_size=504. cond_size=8, max_len=430\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model, put it on the device\n",
    "#model = TransformerDecoder(embed_size, num_layers, num_heads, forward_expansion, dropout_rate, Tt, num_tokens, vocab_size).to(device)\n",
    "print(f'Creating model with embed_size={embed_size}, cond_size={cond_size}')\n",
    "model = TransformerDecoder(embed_size, num_layers, num_heads, forward_expansion, dropout_rate, Tt, num_tokens, vocab_size, cond_size).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58e754a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, inference_cond, Ti, vocab_size, num_tokens, inference_steps, fname) :\n",
    "    model.eval()\n",
    "    mask = generate_mask(Ti, Ti).to(device)\n",
    "    input_data = torch.randint(0, vocab_size, (1, Ti, num_tokens)).to(device)  # Smaller context window for inference\n",
    "    predictions = []\n",
    "\n",
    "    t0 = time.time()\n",
    "    for i in range(inference_steps):  # Generate 100 tokens\n",
    "        if cond_size == 0:\n",
    "            output = model(input_data, None, mask) # step through \n",
    "        else : \n",
    "            output = model(input_data, inference_cond[:, i:Ti+i, :], mask) # step through\n",
    "\n",
    "        # This takes the last vector of the sequence (the new predicted token stack) so has size(b,1,4,1024)\n",
    "        # This it takes the max across the last dimension (scores for each element of the vocabulary (for each of the 4 tokens))\n",
    "        # .max returns a duple of tensors, the first are the max vals (one for each token) and the second are the\n",
    "        #        indices in the range of the vocabulary size. \n",
    "        # THAT IS, the 4 selected \"best\" tokens are taken independently\n",
    "        next_token = output[:, -1, :, :].max(-1)[1]  # Greedy decoding for simplicity\n",
    "        predictions.append(next_token)\n",
    "        input_data = torch.cat([input_data, next_token.unsqueeze(1)], dim=1)[:, 1:]  # Slide window\n",
    "\n",
    "    t1 = time.time()\n",
    "    inf_time = t1-t0\n",
    "    print(f'inference time for {inference_steps} steps, or {inference_steps/86} seconds of sound is {inf_time}' )\n",
    "\n",
    "    dacseq = torch.cat(predictions, dim=0).unsqueeze(0).transpose(1, 2)\n",
    "    if mask == None:\n",
    "        writeDACFile(fname + '_unmasked', dacseq)\n",
    "    else :\n",
    "        writeDACFile(fname, dacseq)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a7baf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot_fvector(bees) = tensor([0., 0., 0., 1., 0., 0., 0.])\n",
      "cvect1 = tensor([0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.5000])\n",
      "cvect2 = tensor([0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.6000])\n",
      "shape of inf_cond is  = torch.Size([1, 947, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformerlw/utils/utils.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  v_tensors = [torch.tensor(vec) for vec in v]\n"
     ]
    }
   ],
   "source": [
    "if cond_size == 0 :\n",
    "    inference_cond = None\n",
    "else : \n",
    "    one_hot_fvector=onehot(testsnd)\n",
    "    print(f'one_hot_fvector({testsnd}) = {one_hot_fvector}')\n",
    "\n",
    "    cvect1=torch.cat((one_hot_fvector, torch.tensor([infminpval])))\n",
    "    cvect2=torch.cat((one_hot_fvector, torch.tensor([infmaxpval])))\n",
    "\n",
    "    steps=[0,Ti+inference_steps//2, Ti+inference_steps]\n",
    "    inference_cond=interpolate_vectors([cvect1,cvect2, cvect1 ], steps) #length must cover staring context window+inf steps\n",
    "\n",
    "    print(f'cvect1 = {cvect1}')\n",
    "    print(f'cvect2 = {cvect2}')\n",
    "\n",
    "    inference_cond=inference_cond.to(device)\n",
    "\n",
    "    print(f'shape of inf_cond is  = {inference_cond.shape}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eaf02d",
   "metadata": {},
   "source": [
    "### <font color='blue'> Train !! \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66a034e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SummaryWriter\n",
    "writer = SummaryWriter(outdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b84df1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 10  loss: 3.401169776916504\n",
      "Validation Loss: 5.4217939376831055\n",
      "\n",
      "EPOCH 20  loss: 1.9584871530532837\n",
      "Validation Loss: 7.181422233581543\n",
      "inference time for 860 steps, or 10.0 seconds of sound is 1.325376272201538\n",
      "\n",
      "EPOCH 30  loss: 1.2794727087020874\n",
      "Validation Loss: 8.759988784790039\n",
      "\n",
      "EPOCH 40  loss: 0.9572731852531433\n",
      "Validation Loss: 9.908573150634766\n",
      "\n",
      "EPOCH 50  loss: 0.7961196303367615\n",
      "Validation Loss: 10.644936561584473\n",
      "inference time for 860 steps, or 10.0 seconds of sound is 1.3506247997283936\n",
      "\n",
      "EPOCH 60  loss: 0.7812052965164185\n",
      "Validation Loss: 11.274145126342773\n",
      "\n",
      "EPOCH 70  loss: 0.6596552133560181\n",
      "Validation Loss: 11.545555114746094\n",
      "inference time for 860 steps, or 10.0 seconds of sound is 1.3205788135528564\n",
      "\n",
      "EPOCH 80  loss: 0.6191518306732178\n",
      "Validation Loss: 11.925834655761719\n",
      "\n",
      "EPOCH 90  loss: 0.5793048143386841\n",
      "Validation Loss: 11.979930877685547\n",
      "\n",
      "EPOCH 100  loss: 0.5267612338066101\n",
      "Validation Loss: 12.145916938781738\n",
      "inference time for 860 steps, or 10.0 seconds of sound is 1.3610467910766602\n",
      "train time for 100 epochs, was 204.33879685401917\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    for batch_idx, (input_data, target_data, cond_data) in enumerate(dataloader):\n",
    "        #print(f\"b{batch_idx} \", end='')\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move inputs and targets to the device\n",
    "        input_data, target_data, cond_data = input_data.to(device), target_data.to(device), cond_data.to(device)\n",
    "        \n",
    "        if cond_size==0 :  #Ignore conditioning data\n",
    "            cond_expanded=None\n",
    "        else : \n",
    "            # for dataset exammples, expand the conditioning info across all time steps before passing to models\n",
    "            cond_expanded = cond_data.unsqueeze(1).expand(-1, input_data.size(1), -1)\n",
    "        \n",
    "        #print(f'    after loading a batch,  input_data.shape is {input_data.shape}, and cond_data.shape is {cond_data.shape}')\n",
    "        #print(f'    after loading a batch,  cond_expanded.shape is {cond_expanded.shape}')\n",
    "\n",
    "        output = model(input_data, cond_expanded, mask)\n",
    "        loss = criterion(output.reshape(-1, vocab_size), target_data.reshape(-1)) # collapses all target_data dimensions into a single dimension\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % ErrorLogRate == 0:\n",
    "        print(f'')\n",
    "        print(f'EPOCH {epoch+1}  ', end='')\n",
    "        print(f'loss: {loss}')\n",
    "        # Log the loss to TensorBoard\n",
    "        writer.add_scalar('Loss/train', loss, epoch)\n",
    "        \n",
    "        if validator_data_dir != None :\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for val_inputs, val_targets, cond_data in validator_dataloader:\n",
    "                    val_inputs, val_targets, cond_data = val_inputs.to(device), val_targets.to(device), cond_data.to(device)\n",
    "                    \n",
    "                    if cond_size==0 :  #Ignore conditioning data\n",
    "                        cond_expanded=None\n",
    "                    else: \n",
    "                        # for dataset exammples, expand the conditioning info across all time steps before passing to models\n",
    "                        cond_expanded = cond_data.unsqueeze(1).expand(-1, input_data.size(1), -1)\n",
    "\n",
    "                    \n",
    "                    val_outputs = model(val_inputs,cond_expanded, mask)\n",
    "                    \n",
    "                    val_loss += criterion(val_outputs.reshape(-1, vocab_size), val_targets.reshape(-1)) # collapses all target_data dimensions into a single dimension\n",
    "                    #val_loss += criterion(val_outputs, val_targets).item()\n",
    "\n",
    "            print(f'Validation Loss: {val_loss / len(validator_dataloader)}')\n",
    "            writer.add_scalar('Loss/validation', val_loss, epoch)\n",
    "            \n",
    "    if (epoch+1) % checkpoint_interval == 0:\n",
    "        lastbasename = outdir+\"/\"+basefname+\"_chkpt_\"+str(epoch+1).zfill(4)\n",
    "        save_model(model, Ti,  lastbasename +\".pth\")\n",
    "        inference(model, inference_cond, Ti, vocab_size, num_tokens, inference_steps, lastbasename) \n",
    "            \n",
    "t1 = time.time()\n",
    "train_time = t1-t0\n",
    "print(f'train time for {num_epochs} epochs, was {train_time}' )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b70a6d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just check that inference attention mask looks right\n",
    "#Actually, the inference mask can be None since we are using a context window only as long as the maximum look-back in the training mask\n",
    "# thats why taking the mask with :TI is upper-triangular. Longer dims would show a banded mask again.\n",
    "foo=mask[:Ti, :Ti]\n",
    "foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384b3825",
   "metadata": {},
   "source": [
    "### <font color='blue'> User DAC2Audio.ipynb  \n",
    "to see and hear your generated audio   \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2530518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo=interpolate_vectors([[1,2,3], [0,0,0], [1,1,1], [3,2,1]], [0,3,4,7])\n",
    "foo\n",
    "foo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd9730ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2.],\n",
      "         [2., 3.],\n",
      "         [3., 4.],\n",
      "         [4., 5.],\n",
      "         [5., 6.]]])\n"
     ]
    }
   ],
   "source": [
    "v = [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]\n",
    "s = [0, 2, 4]\n",
    "result_tensor = interpolate_vectors(v, s)\n",
    "print(result_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5559908b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
