{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb0902d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from utils.utils import generate_mask, load_model, writeDACFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cdf3866",
   "metadata": {},
   "outputs": [],
   "source": [
    "### params\n",
    "cptnum=50\n",
    "checkpoint_dir='runs/scratchpistons'\n",
    "fnamebase=\"pistons.e256.l4.h8\" + \"_chkpt_\" + str(cptnum).zfill(4)\n",
    " \n",
    "checkpoint_path = checkpoint_dir + '/' +  fnamebase  + '.pth' \n",
    "\n",
    "# for saving sound \n",
    "outdir=checkpoint_dir\n",
    "\n",
    "DEVICE='cuda'\n",
    "\n",
    "inference_steps=86*20  #86 frames per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bb87312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memeory on cuda 0 is  25.216745472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_properties(0).total_memory/1e9\n",
    "\n",
    "device = torch.device(DEVICE) # if the docker was started with --gpus all, then can choose here with cuda:0 (or cpu)\n",
    "torch.cuda.device_count()\n",
    "print(f'memeory on cuda 0 is  {torch.cuda.get_device_properties(0).total_memory/1e9}')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f5d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model, Ti, vocab_size, num_codebooks = load_model(checkpoint_path)\n",
    "model.to(device);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39803794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, Ti, vocab_size, num_tokens, inference_steps, fname) :\n",
    "    model.eval()\n",
    "    mask = generate_mask(Ti, Ti).to(device)\n",
    "    input_data = torch.randint(0, vocab_size, (1, Ti, num_tokens)).to(device)  # Smaller context window for inference\n",
    "    predictions = []\n",
    "\n",
    "    t0 = time.time()\n",
    "    for i in range(inference_steps):  # Generate 100 tokens\n",
    "        output = model(input_data, mask)\n",
    "\n",
    "        # This takes the last vector of the sequence (the new predicted token stack) so has size(b,1,4,1024)\n",
    "        # This it takes the max across the last dimension (scores for each element of the vocabulary (for each of the 4 tokens))\n",
    "        # .max returns a duple of tensors, the first are the max vals (one for each token) and the second are the\n",
    "        #        indices in the range of the vocabulary size. \n",
    "        # THAT IS, the 4 selected \"best\" tokens are taken independently\n",
    "        next_token = output[:, -1, :, :].max(-1)[1]  # Greedy decoding for simplicity\n",
    "        predictions.append(next_token)\n",
    "        input_data = torch.cat([input_data, next_token.unsqueeze(1)], dim=1)[:, 1:]  # Slide window\n",
    "\n",
    "    t1 = time.time()\n",
    "    inf_time = t1-t0\n",
    "    print(f'inference time for {inference_steps} steps, or {inference_steps/86} seconds of sound is {inf_time}' )\n",
    "\n",
    "    dacseq = torch.cat(predictions, dim=0).unsqueeze(0).transpose(1, 2)\n",
    "    if mask == None:\n",
    "        writeDACFile(fname + '_unmasked', dacseq)\n",
    "    else :\n",
    "        writeDACFile(fname, dacseq)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db738c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time for 1720 steps, or 20.0 seconds of sound is 4.36799693107605\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "inference(model, Ti, vocab_size, num_codebooks, inference_steps, outdir+\"/\"+ fnamebase+\"_steps_\"+str(inference_steps).zfill(4)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1fd6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
