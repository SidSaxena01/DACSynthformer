{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d87771",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from utils.utils import generate_mask, load_model, writeDACFile, sample_top_n\n",
    "from dataloader.dataset import onehot, getNumClasses, class_name_to_int, int2classname\n",
    "from utils.utils import interpolate_vectors\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "from DACTransformer.DACTransformer import TransformerDecoder\n",
    "from DACTransformer.CondQueryTransformer import ClassConditionedTransformer\n",
    "from DACTransformer.CondKeyTransformer import ClassConditionedKeyTransformer\n",
    "from DACTransformer.PostNormCondDACTransformer import PostNormCondDACTransformerDecoder\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f2e81-6281-48aa-9066-d1935aaaed34",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#For your reference:\n",
    "classes=['pistons', 'wind', 'applause', 'bees']\n",
    "print(f' ------- One hot vectors for classes ----------')\n",
    "for i in range(4):\n",
    "    print(f' {classes[i]} : \\t{onehot(classes[i])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d08550-d689-4f0e-9ddd-57de12846c5e",
   "metadata": {},
   "source": [
    "Morph over a vectors in vsequence lineary for (noramlized) time steps vtimes. Create your sequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f5c883-ab20-4f49-b325-f5221a1b9cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vsequence=[\n",
    "    torch.tensor([1., 0., 0., 0., 0., 0., 0., 0.5]),\n",
    "    torch.tensor([1., 0., 0., 0., 0., 0., 0., 0.5]),  \n",
    "    torch.tensor([1., 0., 1., 0., 0., 0., 0., 0.5]),\n",
    "    torch.tensor([1., 0., 1., 0., 0., 0., 0., 0.5]),\n",
    "    torch.tensor([0., 0., 1., 0., 0., 0., 0., 0.5]),\n",
    "    torch.tensor([0., 0., 1., 0., 0., 0., 0., 0.5])\n",
    "]\n",
    "vtimes=[0,.2,.4,.6,.8, 1] # must be the same length as the number of break points in vsequence\n",
    "morphname='pistons.applause.doubling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### params\n",
    "experiment_name = \"01.20PostNormCond_2_\" \n",
    "checkpoint_dir = 'runs' + '/' + experiment_name\n",
    "cptnum =  600 #params['num_epochs'] # 300 #(must be in the checkpoint directory)\n",
    "\n",
    "# Load YAML file\n",
    "with open(checkpoint_dir + '/' + 'params.yaml', 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "    \n",
    "TransformerClass =  globals().get(params['TransformerClass'])  \n",
    "print(f\"using TransformerClass = {params['TransformerClass']}\") \n",
    "print(f' and TransformerClass is class object {TransformerClass}')\n",
    "\n",
    "cond_size = 8 # num_classes + num params - not a FREE parameter!\n",
    "\n",
    "embed_size = params['tblock_input_size'] -cond_size # 240 #32  # embed_size must be divisible by num_heads and by num tokens\n",
    "print(f'embed_size is {embed_size}')\n",
    "\n",
    "\n",
    "fnamebase='out' + '.e' + str(params['tblock_input_size']-cond_size) + '.l' + str(params['num_layers']) + '.h' + str(params['num_heads']) + '_chkpt_' + str(cptnum).zfill(4) \n",
    "checkpoint_path = checkpoint_dir + '/' +  fnamebase  + '.pth' \n",
    "\n",
    "# for saving sound \n",
    "outdir=checkpoint_dir\n",
    "SAVEWAV=True\n",
    "\n",
    "DEVICE='cpu' #####################################################''cuda'\n",
    "\n",
    "inference_steps=86*20  #86 frames per second\n",
    "\n",
    "\n",
    "# Values for interpolating the parameter value (start at minpval, up to maxpval, and then back)\n",
    "minpval=0\n",
    "maxpval=1\n",
    "topn=1024 # sample from the top n logits\n",
    "\n",
    "\n",
    "print(f'checkpoint_path = {checkpoint_path}, fnamebase = {fnamebase}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e01007",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEVICE == 'cuda' :\n",
    "    torch.cuda.device_count()\n",
    "    torch.cuda.get_device_properties(0).total_memory/1e9\n",
    "\n",
    "    device = torch.device(DEVICE) # if the docker was started with --gpus all, then can choose here with cuda:0 (or cpu)\n",
    "    torch.cuda.device_count()\n",
    "    print(f'memeory on cuda 0 is  {torch.cuda.get_device_properties(0).total_memory/1e9}')\n",
    "else :\n",
    "    device=DEVICE\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c897ef-cea3-4f44-98f8-55d34be9c578",
   "metadata": {},
   "source": [
    "# Run Transformer to generate a DAC-coded audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5244dd",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def inference(model, inference_cond, Ti_context_length, vocab_size, num_tokens, inference_steps, fname) :\n",
    "    model.eval()\n",
    "    mask = generate_mask(Ti_context_length, Ti_context_length).to(device)\n",
    "\n",
    "    # The \"input data\" is random with a sequence length equal to the context length (and the mask) which is used \n",
    "    # to generate the first step of the output.It is not included in the output.\n",
    "    input_data = torch.randint(0, vocab_size, (1, Ti_context_length, num_tokens)).to(device)  # Smaller context window for inference\n",
    "    #Extend the first conditional vector to cover the \"input\" which is of length Ti_context_length\n",
    "    inference_cond = torch.cat([inference_cond[:, :1, :].repeat(1, Ti_context_length, 1), inference_cond], dim=1)\n",
    "    predictions = []\n",
    "\n",
    "    print(f' in the inference function, the shape of input_data is {input_data.shape} and the shape of the inference_cond is {inference_cond.shape}')\n",
    "\n",
    "    t0 = time.time()\n",
    "    for i in range(inference_steps):  # \n",
    "        if cond_size == 0:\n",
    "            output = model(input_data, None, mask) # step through \n",
    "        else : \n",
    "            output = model(input_data, inference_cond[:, i:Ti_context_length+i, :], mask) # step through\n",
    "\n",
    "        # This takes the last vector of the sequence (the new predicted token stack) so has size(b,steps,4,1024)\n",
    "        # This it takes the max across the last dimension (scores for each element of the vocabulary (for each of the 4 tokens))\n",
    "        # .max returns a duple of tensors, the first are the max vals (one for each token) and the second are the\n",
    "        #        indices in the range of the vocabulary size. \n",
    "        # THAT IS, the selected \"best\" tokens (one for each codebook) are taken independently\n",
    "        ########################### next_token = output[:, -1, :, :].max(-1)[1]  # Greedy decoding for simplicity\n",
    "        next_token = sample_top_n(output[:, -1, :, :],2) # top 1 would be the same as max in the comment line above\n",
    "            \n",
    "        #print(f'next_token: {next_token} which had a top 1 logit value of {sample_top_n(output[:, -1, :, :],1)}')\n",
    "        #print(f'                                       and a had a top 2 logit value of {sample_top_n(output[:, -1, :, :],2)}')\n",
    "                                                                           \n",
    "                                                                           \n",
    "        \n",
    "        predictions.append(next_token)\n",
    "        input_data = torch.cat([input_data, next_token.unsqueeze(1)], dim=1)[:, 1:]  # Slide window\n",
    "\n",
    "    t1 = time.time()\n",
    "    inf_time = t1-t0\n",
    "    print(f'inference time for {inference_steps} steps, or {inference_steps/86} seconds of sound is {inf_time}' )\n",
    "\n",
    "    dacseq = torch.cat(predictions, dim=0).unsqueeze(0).transpose(1, 2)\n",
    "    if mask == None:\n",
    "        writeDACFile(fname + '_unmasked', dacseq)\n",
    "    else :\n",
    "        writeDACFile(fname, dacseq)   \n",
    "\n",
    "    print(f'dacseq shape written to file is of shape {dacseq.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0117837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' About to call load_model with TransformerClass = {TransformerClass}')\n",
    "model, Ti_context_length, vocab_size, num_codebooks, cond_size = load_model(checkpoint_path,  TransformerClass, DEVICE)\n",
    "print(f'Mode loaded, context_length (Ti_context_length) = {Ti_context_length}')\n",
    "# Count the number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {num_params}')\n",
    "\n",
    "\n",
    "model.to(device);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa8d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for snum in range(1) : \n",
    "    if cond_size == 0 :\n",
    "        inference_cond = None\n",
    "    else : \n",
    "\n",
    "        inference_cond=interpolate_vectors(vsequence, [round(x * inference_steps) for x in vtimes]) #length must cover staring context window+inf steps\n",
    "\n",
    "        \n",
    "        # Make a picture --------------------------------------------------------------\n",
    "        # Extract the 2D array of shape [n, m]\n",
    "        data = inference_cond[0]\n",
    "        # Find components that change over time\n",
    "        changing_indices = [i for i in range(cond_size) if not torch.all(data[:, i] == data[0, i])]\n",
    "\n",
    "        # Plot the changing components\n",
    "        plt.figure(figsize=(10, 3))\n",
    "\n",
    "        for i in changing_indices:\n",
    "            if i != 7 :\n",
    "                plt.plot(data[:, i], label=f'{int2classname[i]} ({i})')\n",
    "            else : \n",
    "                plt.plot(data[:, i], label=f'Parameter ({i})', linestyle='--')\n",
    "\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Component Values')\n",
    "        plt.title(f' {morphname}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        inference_cond=inference_cond.to(device)\n",
    "        print(f'shape of inf_cond is  = {inference_cond.shape}') \n",
    "\n",
    "        \n",
    "        outfname=outdir+\"/\"+ \"dacs\" + \"/\" +  morphname + '_chkpt_' + str(cptnum).zfill(4) +  \"_steps_\"+str(inference_steps).zfill(4) +'.topn_'+ f\"{topn:04d}\"\n",
    "        print(f'outfname is {outfname}')\n",
    "        inference(model, inference_cond, Ti_context_length, vocab_size, num_codebooks, inference_steps, outfname ) \n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15149fd8-e9f6-4f70-84a0-befd13629c18",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; height: 20px; background-color: black;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e5b6c-6ec5-419d-a60c-5e8594f98375",
   "metadata": {},
   "source": [
    "# Read the dacfile, decode it to audio, and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3dee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dac\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b557d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first time you do this, it can take a while. Go get coffee. After that, it uses a cached version\n",
    "dacmodel_path = dac.utils.download(model_type=\"44khz\") \n",
    "print(f'The DAC decoder is in {dacmodel_path}')\n",
    "with torch.no_grad():\n",
    "    dacmodel = dac.DAC.load(dacmodel_path)\n",
    "\n",
    "    dacmodel.to(device); #wanna see the model? remove the semicolon\n",
    "    dacmodel.eval();  # need to be \"in eval mode\" in order to set the number of quantizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3999f349-0625-4500-b868-5c1c53f7bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------  derived ------ don't change these \n",
    "selected_file=outfname + \".dac\"\n",
    "print(f' selected_file is {selected_file}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    dacfile = dac.DACFile.load(selected_file)\n",
    "    # FIRST - Decompress it back to an AudioSignal\\ from codes to z (1024) to signal   \n",
    "    print(f'dacfile.codes shape is: {dacfile.codes.shape}')\n",
    "    t0=time.time()\n",
    "    asig=dacmodel.decompress(dacfile)\n",
    "    t1=time.time()\n",
    "    \n",
    "    inf_time = t1-t0\n",
    "    print(f'decompress time for {asig.audio_data.shape[2]/44100} seconds of sound is {inf_time}' )\n",
    "    print(f'asig.audio_data.shape[2] is {asig.audio_data.shape[2]}')\n",
    "    \n",
    "    asig.cpu().widget()\n",
    "    asig.save_image(outfname + \".jpg\")\n",
    "    asig.audio_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b679c-b5c1-4e12-ba69-e45fc8ef72e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = asig.samples.view(-1).numpy()\n",
    "if SAVEWAV :  \n",
    "    sf.write(outfname + \".wav\", adata, 44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969287fb-215e-4db8-ab94-8dfe3be4c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Audio display\n",
    "plt.plot(adata)\n",
    "# Audio player\n",
    "ipd.Audio(adata, rate=44100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c2b6ff-75bc-4948-81cf-bc28bd6a3d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
