{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from utils.utils import generate_mask, load_model, writeDACFile\n",
    "from dataloader.dataset import onehot, getNumClasses\n",
    "from utils.utils import interpolate_vectors\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "from DACTransformer.DACTransformer import TransformerDecoder\n",
    "from DACTransformer.CondQueryTransformer import ClassConditionedTransformer\n",
    "from DACTransformer.CondKeyTransformer import ClassConditionedKeyTransformer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b29b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "### params\n",
    "experiment_name = \"06.30_Keysmall\" \n",
    "checkpoint_dir = 'runs' + '/' + experiment_name\n",
    "cptnum =  500 #params['num_epochs'] # 300 #(must be in the checkpoint directory)\n",
    "\n",
    "# will morph between infsnd1[i] and infsnd2[i] (and back)\n",
    "infsnd1 = ['pistons', 'wind', 'applause', 'bees'] # used for inference\n",
    "#infsnd2 = ['pistons' , 'wind',   'applause', 'bees'] # used for inference\n",
    "infsnd2 = ['wind',   'applause', 'bees', 'pistons' ] # used for inference\n",
    "\n",
    "# Load YAML file\n",
    "with open(checkpoint_dir + '/' + 'params.yaml', 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "    \n",
    "TransformerClass =  globals().get(params['TransformerClass'])  \n",
    "print(f\"using TransformerClass = {params['TransformerClass']}\") \n",
    "print(f' and TransformerClass is class object {TransformerClass}')\n",
    "\n",
    "cond_size = 8 # num_classes + num params - not a FREE parameter!\n",
    "\n",
    "embed_size = params['tblock_input_size'] -cond_size # 240 #32  # embed_size must be divisible by num_heads and by num tokens\n",
    "print(f'embed_size is {embed_size}')\n",
    "\n",
    "\n",
    "fnamebase='out' + '.e' + str(params['tblock_input_size']-cond_size) + '.l' + str(params['num_layers']) + '.h' + str(params['num_heads']) + '_chkpt_' + str(cptnum).zfill(4) \n",
    "checkpoint_path = checkpoint_dir + '/' +  fnamebase  + '.pth' \n",
    "\n",
    "# for saving sound \n",
    "outdir=checkpoint_dir\n",
    "\n",
    "DEVICE='cpu' #####################################################''cuda'\n",
    "\n",
    "inference_steps=86*20  #86 frames per second\n",
    "\n",
    "minpval=0\n",
    "maxpval=1\n",
    "\n",
    "print(f'checkpoint_path = {checkpoint_path}, fnamebase = {fnamebase}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dca525",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_properties(0).total_memory/1e9\n",
    "\n",
    "device = torch.device(DEVICE) # if the docker was started with --gpus all, then can choose here with cuda:0 (or cpu)\n",
    "torch.cuda.device_count()\n",
    "print(f'memeory on cuda 0 is  {torch.cuda.get_device_properties(0).total_memory/1e9}')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8683942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes the output vector of activations (\"logits\") across the vocabulary, chooses the n maximally activated,\n",
    "# does a softmax, and chooses according to their probablity\n",
    "\n",
    "def select_top_n(logits, n):\n",
    "    \"\"\"\n",
    "    Selects the next token from the top n logits using softmax probabilities.\n",
    "    \n",
    "    Parameters:\n",
    "    logits (torch.Tensor): The logits tensor (1D tensor).\n",
    "    n (int): The number of top logits to consider.\n",
    "\n",
    "    Returns:\n",
    "    int: The index of the selected token.\n",
    "    \"\"\"\n",
    "    # Ensure n does not exceed the length of logits\n",
    "    n = min(n, logits.size(0))\n",
    "\n",
    "    # Step 1: Get the indices of the top n logits\n",
    "    top_n_logits, top_n_indices = torch.topk(logits, n)\n",
    "    \n",
    "    # Step 2: Apply softmax to the top n logits\n",
    "    top_n_probs = torch.softmax(top_n_logits, dim=0)\n",
    "    \n",
    "    # Step 3: Choose a token from the top n logits according to the probabilities\n",
    "    selected_index_within_top_n = torch.multinomial(top_n_probs, 1).item()\n",
    "    \n",
    "    # Step 4: Map the selected index within top n back to the original index\n",
    "    selected_token_index = top_n_indices[selected_index_within_top_n].item()\n",
    "    \n",
    "    return selected_token_index\n",
    "\n",
    "# Example usage\n",
    "logits = torch.tensor([1.0, 2.0, 3.0, 10.0, 0.5, 0.1, 0.2, 5.0])\n",
    "n = 3\n",
    "selected_token_index = select_top_n(logits, n)\n",
    "print(f\"Selected token index: {selected_token_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676583ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, inference_cond, Ti, vocab_size, num_tokens, inference_steps, fname) :\n",
    "    model.eval()\n",
    "    mask = generate_mask(Ti, Ti).to(device)\n",
    "    input_data = torch.randint(0, vocab_size, (1, Ti, num_tokens)).to(device)  # Smaller context window for inference\n",
    "    predictions = []\n",
    "\n",
    "    t0 = time.time()\n",
    "    for i in range(inference_steps):  # Generate 100 tokens\n",
    "        if cond_size == 0:\n",
    "            output = model(input_data, None, mask) # step through \n",
    "        else : \n",
    "            output = model(input_data, inference_cond[:, i:Ti+i, :], mask) # step through\n",
    "\n",
    "        # This takes the last vector of the sequence (the new predicted token stack) so has size(b,1,4,1024)\n",
    "        # This it takes the max across the last dimension (scores for each element of the vocabulary (for each of the 4 tokens))\n",
    "        # .max returns a duple of tensors, the first are the max vals (one for each token) and the second are the\n",
    "        #        indices in the range of the vocabulary size. \n",
    "        # THAT IS, the 4 selected \"best\" tokens are taken independently\n",
    "        next_token = output[:, -1, :, :].max(-1)[1]  # Greedy decoding for simplicity\n",
    "        predictions.append(next_token)\n",
    "        input_data = torch.cat([input_data, next_token.unsqueeze(1)], dim=1)[:, 1:]  # Slide window\n",
    "\n",
    "    t1 = time.time()\n",
    "    inf_time = t1-t0\n",
    "    print(f'inference time for {inference_steps} steps, or {inference_steps/86} seconds of sound is {inf_time}' )\n",
    "\n",
    "    dacseq = torch.cat(predictions, dim=0).unsqueeze(0).transpose(1, 2)\n",
    "    if mask == None:\n",
    "        writeDACFile(fname + '_unmasked', dacseq)\n",
    "    else :\n",
    "        writeDACFile(fname, dacseq)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eea54ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' About to call load_model with TransformerClass = {TransformerClass}')\n",
    "model, Ti, vocab_size, num_codebooks, cond_size = load_model(checkpoint_path,  TransformerClass)\n",
    "\n",
    "# Count the number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {num_params}')\n",
    "\n",
    "\n",
    "model.to(device);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de7d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for snum in range(len(infsnd1)) : \n",
    "    if cond_size == 0 :\n",
    "        inference_cond = None\n",
    "    else : \n",
    "        one_hot_fvector1=onehot(infsnd1[snum]) #The parameter evolution over time will be the same for all sounds\n",
    "        one_hot_fvector2=onehot(infsnd2[snum]) #The parameter evolution over time will be the same for all sounds\n",
    "\n",
    "        cvect1=torch.cat((one_hot_fvector1, torch.tensor([minpval])))\n",
    "        cvect2=torch.cat((one_hot_fvector2, torch.tensor([maxpval])))\n",
    "\n",
    "        steps=[0,Ti+1*inference_steps//5, Ti+2*inference_steps//5, Ti+3*inference_steps//5, Ti+4*inference_steps//5, Ti+inference_steps]\n",
    "        inference_cond=interpolate_vectors([cvect1,cvect1, cvect2, cvect2, cvect1, cvect1 ], steps) #length must cover staring context window+inf steps\n",
    "\n",
    "        # Extract the 2D array of shape [n, m]\n",
    "        data = inference_cond[0]\n",
    "\n",
    "        # Find components that change over time\n",
    "        changing_indices = [i for i in range(cond_size) if not torch.all(data[:, i] == data[0, i])]\n",
    "\n",
    "        # Format the arrays as strings\n",
    "        cvect1_str = ', '.join(map(str, cvect1.tolist()))\n",
    "        cvect2_str = ', '.join(map(str, cvect2.tolist()))\n",
    "\n",
    "        # Plot the changing components\n",
    "        plt.figure(figsize=(10, 3))\n",
    "\n",
    "        for i in changing_indices:\n",
    "            if i != 7 :\n",
    "                plt.plot(data[:, i], label=f'Component {i}')\n",
    "            else : \n",
    "                plt.plot(data[:, i], label=f'Component {i}', linestyle='--')\n",
    "\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Component Values')\n",
    "        plt.title(f' cvect1 = [{cvect1_str}] \\ncvect2 = [{cvect2_str}]')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        inference_cond=inference_cond.to(device)\n",
    "        print(f'shape of inf_cond is  = {inference_cond.shape}') \n",
    "\n",
    "        \n",
    "        outfname=outdir+\"/\"+ \"dacs\" + \"/\" +  infsnd1[snum] + \".\" + infsnd2[snum] + '_chkpt_' + str(cptnum).zfill(4) +  \"_steps_\"+str(inference_steps).zfill(4)+'.minpval_'+ f\"{minpval:01.2f}\" +'.maxpval_'+ f\"{maxpval:01.2f}\"\n",
    "        print(f'outfname is {outfname}')\n",
    "        inference(model, inference_cond, Ti, vocab_size, num_codebooks, inference_steps, outfname ) \n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bff5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
