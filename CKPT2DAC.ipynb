{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "775dfab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from utils.utils import generate_mask, load_model, writeDACFile\n",
    "from dataloader.dataset import onehot, getNumClasses\n",
    "from utils.utils import interpolate_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e98c713",
   "metadata": {},
   "outputs": [],
   "source": [
    "### params\n",
    "cptnum=300\n",
    "testsnd=\"bees\"\n",
    "checkpoint_dir='runs/newscratch'+ testsnd + \"cond8\"\n",
    "fnamebase=testsnd+'.e248.l4.h8_chkpt_'+str(cptnum).zfill(4)\n",
    " \n",
    "checkpoint_path = checkpoint_dir + '/' +  fnamebase  + '.pth' \n",
    "\n",
    "# for saving sound \n",
    "outdir=checkpoint_dir\n",
    "\n",
    "DEVICE='cuda'\n",
    "\n",
    "inference_steps=86*20  #86 frames per second\n",
    "\n",
    "minpval=0\n",
    "maxpval=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4047ee6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memeory on cuda 0 is  25.216745472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_properties(0).total_memory/1e9\n",
    "\n",
    "device = torch.device(DEVICE) # if the docker was started with --gpus all, then can choose here with cuda:0 (or cpu)\n",
    "torch.cuda.device_count()\n",
    "print(f'memeory on cuda 0 is  {torch.cuda.get_device_properties(0).total_memory/1e9}')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c586106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get a coder with embed_size=248. cond_size=8, max_len=430\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model, Ti, vocab_size, num_codebooks, cond_size = load_model(checkpoint_path)\n",
    "model.to(device);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdfca50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot_fvector(bees) = tensor([0., 0., 0., 1., 0., 0., 0.])\n",
      "cvect1 = tensor([0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "cvect2 = tensor([0., 0., 0., 1., 0., 0., 0., 1.])\n",
      "shape of inf_cond is  = torch.Size([1, 1893, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dacsynthformer/utils/utils.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  v_tensors = [torch.tensor(vec) for vec in v]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if cond_size == 0 :\n",
    "    inference_cond = None\n",
    "else : \n",
    "    one_hot_fvector=onehot(testsnd)\n",
    "    print(f'one_hot_fvector({testsnd}) = {one_hot_fvector}')\n",
    "\n",
    "    cvect1=torch.cat((one_hot_fvector, torch.tensor([minpval])))\n",
    "    cvect2=torch.cat((one_hot_fvector, torch.tensor([maxpval])))\n",
    "\n",
    "    steps=[0,Ti+1*inference_steps//5, Ti+2*inference_steps//5, Ti+3*inference_steps//5, Ti+4*inference_steps//5, Ti+inference_steps]\n",
    "    inference_cond=interpolate_vectors([cvect1,cvect1, cvect2, cvect2, cvect1, cvect1 ], steps) #length must cover staring context window+inf steps\n",
    "\n",
    "    print(f'cvect1 = {cvect1}')\n",
    "    print(f'cvect2 = {cvect2}')\n",
    "\n",
    "    inference_cond=inference_cond.to(device)\n",
    "\n",
    "    print(f'shape of inf_cond is  = {inference_cond.shape}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f886e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, inference_cond, Ti, vocab_size, num_tokens, inference_steps, fname) :\n",
    "    model.eval()\n",
    "    mask = generate_mask(Ti, Ti).to(device)\n",
    "    input_data = torch.randint(0, vocab_size, (1, Ti, num_tokens)).to(device)  # Smaller context window for inference\n",
    "    predictions = []\n",
    "\n",
    "    t0 = time.time()\n",
    "    for i in range(inference_steps):  # Generate 100 tokens\n",
    "        if cond_size == 0:\n",
    "            output = model(input_data, None, mask) # step through \n",
    "        else : \n",
    "            output = model(input_data, inference_cond[:, i:Ti+i, :], mask) # step through\n",
    "\n",
    "        # This takes the last vector of the sequence (the new predicted token stack) so has size(b,1,4,1024)\n",
    "        # This it takes the max across the last dimension (scores for each element of the vocabulary (for each of the 4 tokens))\n",
    "        # .max returns a duple of tensors, the first are the max vals (one for each token) and the second are the\n",
    "        #        indices in the range of the vocabulary size. \n",
    "        # THAT IS, the 4 selected \"best\" tokens are taken independently\n",
    "        next_token = output[:, -1, :, :].max(-1)[1]  # Greedy decoding for simplicity\n",
    "        predictions.append(next_token)\n",
    "        input_data = torch.cat([input_data, next_token.unsqueeze(1)], dim=1)[:, 1:]  # Slide window\n",
    "\n",
    "    t1 = time.time()\n",
    "    inf_time = t1-t0\n",
    "    print(f'inference time for {inference_steps} steps, or {inference_steps/86} seconds of sound is {inf_time}' )\n",
    "\n",
    "    dacseq = torch.cat(predictions, dim=0).unsqueeze(0).transpose(1, 2)\n",
    "    if mask == None:\n",
    "        writeDACFile(fname + '_unmasked', dacseq)\n",
    "    else :\n",
    "        writeDACFile(fname, dacseq)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "725053b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outfname is runs/newscratchbeescond8/dacs/bees.e248.l4.h8_chkpt_0300_steps_1720.minpval_0.00.maxpval_1.00\n",
      "inference time for 1720 steps, or 20.0 seconds of sound is 5.111999034881592\n",
      "Just so ya know, I had to create the path to save the file\n"
     ]
    }
   ],
   "source": [
    "outfname=outdir+\"/\"+ \"dacs\" + \"/\" +  fnamebase+\"_steps_\"+str(inference_steps).zfill(4)+'.minpval_'+ f\"{minpval:01.2f}\" +'.maxpval_'+ f\"{maxpval:01.2f}\"\n",
    "print(f'outfname is {outfname}')\n",
    "inference(model, inference_cond, Ti, vocab_size, num_codebooks, inference_steps, outfname ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c322215b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
